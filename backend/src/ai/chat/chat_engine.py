"""
Chat Engine - LangChain ê¸°ë°˜ ì±„íŒ… ì—”ì§„

ì´ ëª¨ë“ˆì˜ ëª©ì :
- LangChainì„ ì‚¬ìš©í•œ ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬
- íƒ€ë¡œ ë§ˆìŠ¤í„° í˜ë¥´ì†Œë‚˜ ì„¤ì •
- ê¸°ì¡´ AI Orchestratorì™€ í†µí•©
"""
from typing import List, Dict, Any, Optional
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

from src.core.config import settings
from src.core.logging import get_logger
from src.ai.chat.memory_manager import MemoryManager
from src.ai.orchestrator import AIOrchestrator
from src.ai.provider_loader import load_providers_from_settings

logger = get_logger(__name__)


class ChatEngine:
    """
    ì±„íŒ… ì—”ì§„

    LangChainì„ ì‚¬ìš©í•˜ì—¬ ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """

    def __init__(self, conversation_id: str, user_id: str):
        """
        Initialize Chat Engine

        Args:
            conversation_id: ëŒ€í™” ID
            user_id: ì‚¬ìš©ì ID
        """
        self.conversation_id = conversation_id
        self.user_id = user_id
        self.memory_manager = MemoryManager(conversation_id, user_id)

        # System prompt for tarot master
        self.system_prompt = self._load_system_prompt()

        # LLM ì„¤ì •
        self.llm = self._get_llm()

        # Chat prompt template
        self.prompt_template = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ])

        # LCEL Chain (LangChain Expression Language - recommended for v1.0+)
        self.chain = self.prompt_template | self.llm

    def _load_system_prompt(self) -> str:
        """íƒ€ë¡œ ë§ˆìŠ¤í„° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
        try:
            prompt_path = "backend/prompts/system/tarot_expert.txt"
            with open(prompt_path, "r", encoding="utf-8") as f:
                base_prompt = f.read()

            # ì±„íŒ…ìš© ì¶”ê°€ ì§€ì‹œì‚¬í•­
            chat_instructions = """
            
## Chat Mode Instructions

You are now in a conversational chat mode. Your role is to:
1. Have natural, empathetic conversations with users about their concerns
2. Provide tarot-related guidance and insights when appropriate
3. Remember previous conversations and maintain context
4. Be warm, supportive, and encouraging

## IMPORTANT: Card Selection Rules
- You MUST NEVER draw or select tarot cards yourself
- You MUST NEVER describe specific cards or their meanings unless the user has already selected cards
- When a user asks for a tarot reading, fortune, or card interpretation, you should:
  1. Ask them to select their cards first
  2. Say something like: "ì¢‹ì•„ìš”! ì¹´ë“œë¥¼ ì„ íƒí•´ì£¼ì‹œë©´ í•´ì„í•´ ë“œë¦´ê²Œìš”. ì•„ë˜ì—ì„œ ì¹´ë“œë¥¼ ê³¨ë¼ì£¼ì„¸ìš”."
  3. Wait for them to select cards before providing any interpretation

## Response Guidelines
- When user mentions: ìš´ì„¸, íƒ€ë¡œ, ë¦¬ë”©, ì¹´ë“œ, fortune, reading, cards
- DO NOT start a reading or describe cards
- Instead, guide them to select cards with the card selector below
- Example responses:
  - "ë¬¼ë¡ ì´ì£ ! ì¹´ë“œë¥¼ ì„ íƒí•´ì£¼ì‹œë©´ ë°”ë¡œ í•´ì„í•´ ë“œë¦´ê²Œìš”. ğŸŒŸ"
  - "ë„¤, íƒ€ë¡œ ë¦¬ë”©ì„ í•´ë“œë¦´ê²Œìš”. ë¨¼ì € ì•„ë˜ì—ì„œ ì¹´ë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”!"
  - "ì¢‹ì•„ìš”! ì–´ë–¤ ì¹´ë“œê°€ ë‚˜ì˜¬ì§€ ê¸°ëŒ€ë˜ë„¤ìš”. ì¹´ë“œë¥¼ ê³¨ë¼ì£¼ì„¸ìš”."

Keep responses conversational and natural, not overly formal.
"""
            return base_prompt + chat_instructions
        except Exception as e:
            logger.warning(f"Failed to load system prompt: {e}, using default")
            return "You are a professional tarot reader with 20 years of experience. Be empathetic, warm, and supportive."

    def _get_llm(self):
        """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        provider = settings.DEFAULT_AI_PROVIDER.lower()

        if provider == "openai":
            if not settings.OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEY is not set. Please set it in your .env file or environment variables.")
            return ChatOpenAI(
                model=settings.OPENAI_MODEL,
                temperature=0.7,
                api_key=settings.OPENAI_API_KEY,
            )
        elif provider == "anthropic":
            if not settings.ANTHROPIC_API_KEY:
                raise ValueError("ANTHROPIC_API_KEY is not set. Please set it in your .env file or environment variables.")
            return ChatAnthropic(
                model=settings.ANTHROPIC_MODEL,
                temperature=0.7,
                api_key=settings.ANTHROPIC_API_KEY,
            )
        elif provider == "gemini":
            if not settings.GEMINI_API_KEY:
                raise ValueError("GEMINI_API_KEY is not set. Please set it in your .env file or environment variables.")
            return ChatGoogleGenerativeAI(
                model=settings.GEMINI_MODEL,
                temperature=0.7,
                google_api_key=settings.GEMINI_API_KEY,
            )
        else:
            # Default to OpenAI
            if not settings.OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEY is not set. Please set it in your .env file or environment variables.")
            return ChatOpenAI(
                model=settings.OPENAI_MODEL,
                temperature=0.7,
                api_key=settings.OPENAI_API_KEY,
            )

    async def initialize(self):
        """ì±„íŒ… ì—”ì§„ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ë¡œë“œ)"""
        await self.memory_manager.load_memory()

    async def chat(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±

        Args:
            user_input: ì‚¬ìš©ì ë©”ì‹œì§€

        Returns:
            AI ì‘ë‹µ
        """
        try:
            # ë©”ëª¨ë¦¬ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
            memory_vars = self.memory_manager.get_memory_variables()
            chat_history = memory_vars.get("chat_history", [])

            # LCEL Chain ì‹¤í–‰
            response = await self.chain.ainvoke({
                "input": user_input,
                "chat_history": chat_history,
            })

            # LCEL returns AIMessage directly, extract content
            from langchain_core.messages import AIMessage
            if isinstance(response, AIMessage):
                ai_response = response.content
                # ë©”ëª¨ë¦¬ì— ì¶”ê°€ (ì´ë¯¸ AIMessage ê°ì²´ì´ë¯€ë¡œ ì§ì ‘ ì‚¬ìš©)
                self.memory_manager.short_term_memory.chat_memory.add_message(response)
            else:
                # Fallback for unexpected response types
                ai_response = str(response)
                self.memory_manager.short_term_memory.chat_memory.add_message(
                    AIMessage(content=ai_response)
                )

            return ai_response

        except Exception as e:
            logger.error(f"[ChatEngine] Failed to generate response: {e}")
            raise

    async def suggest_tarot_reading(self, context: str) -> bool:
        """
        ë§¥ë½ì„ ë¶„ì„í•˜ì—¬ íƒ€ë¡œ ë¦¬ë”© ì œì•ˆ ì—¬ë¶€ ê²°ì •

        Args:
            context: í˜„ì¬ ëŒ€í™” ë§¥ë½

        Returns:
            íƒ€ë¡œ ë¦¬ë”© ì œì•ˆ ì—¬ë¶€
        """
        # íƒ€ë¡œ/ìš´ì„¸ ì§ì ‘ ìš”ì²­ í‚¤ì›Œë“œ (ë†’ì€ ìš°ì„ ìˆœìœ„)
        tarot_request_keywords = [
            "ìš´ì„¸", "íƒ€ë¡œ", "ë¦¬ë”©", "ì¹´ë“œ", "ì ", "fortune", "reading", "card",
            "ë½‘ì•„", "ë´ì¤˜", "ë´ì£¼ì„¸ìš”", "í•´ì¤˜", "í•´ì£¼ì„¸ìš”", "ì•Œë ¤ì¤˜"
        ]
        
        context_lower = context.lower()
        
        # íƒ€ë¡œ ê´€ë ¨ ì§ì ‘ ìš”ì²­ì¸ ê²½ìš° ë°”ë¡œ True
        for keyword in tarot_request_keywords:
            if keyword in context_lower:
                return True

        return False

    async def generate_conversation_title(self, first_message: str) -> str:
        """
        ì²« ë©”ì‹œì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€í™” ì œëª© ìë™ ìƒì„±

        Args:
            first_message: ì‚¬ìš©ìì˜ ì²« ë©”ì‹œì§€

        Returns:
            ìƒì„±ëœ ëŒ€í™” ì œëª© (ìµœëŒ€ 40ì)
        """
        try:
            # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì œëª© ìƒì„±
            title_prompt = f"""Based on this user message, generate a short, concise conversation title (max 40 characters).
The title should capture the main topic or concern.
Respond with ONLY the title, nothing else.
Use Korean if the message is in Korean, English if the message is in English.

User message: {first_message}

Title:"""

            response = await self.llm.ainvoke(title_prompt)
            
            # Extract content from response
            from langchain_core.messages import AIMessage
            if isinstance(response, AIMessage):
                title = response.content.strip()
            else:
                title = str(response).strip()

            # ìµœëŒ€ ê¸¸ì´ ì œí•œ
            if len(title) > 40:
                title = title[:37] + "..."

            # ë”°ì˜´í‘œ ì œê±°
            title = title.strip('"\'')

            logger.info(f"[ChatEngine] Generated title: {title}")
            return title

        except Exception as e:
            logger.error(f"[ChatEngine] Failed to generate title: {e}")
            # ì‹¤íŒ¨ ì‹œ ì²« ë©”ì‹œì§€ì˜ ì•ë¶€ë¶„ ì‚¬ìš©
            if len(first_message) > 30:
                return first_message[:27] + "..."
            return first_message

